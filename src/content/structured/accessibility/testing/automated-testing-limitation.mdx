---
path: "/accessibility/testing/automated-testing-limitation"

navPriority: 3

date: "2024-04-15"

title: "Limitations of automated testing"

subTitle: "Automated accessibility testing is essential but it has limitations and only finds between 30-50% of accessibility problems."

contribute: "https://github.com/mi6/ic-design-system/tree/main/src/content/structured/accessibility/testing/automated-testing-limitation.mdx"
---

## False positives and false assurance

Passing automated accessibility tests doesn't mean you app is accessible, it just means that no common issues were found.

A limitation with automated accessibility testing is that results often need someone to ensure the result is correct. This includes checking for false positives and false negatives.

## WCAG and context

Checks in automated accessibility testing tools have a yes or no outcome. The biggest limitation of these automated tools is because they can't test for context.

<p>
  Deque’s{" "}
  <ic-link
    target="_blank"
    href="https://dequeuniversity.com/checklists/web/guide"
    rel="noreferer noopener nofollow"
  >
    Accessibility Developers' Guide
  </ic-link>{" "}
  shows what type of testing is needed to meet{" "}
  <ic-link
    target=""
    href="/accessibility/requirement/wcag"
    rel="noreferer noopener nofollow"
  >
    WCAG 2.2 Level AA
  </ic-link>
  .
</p>

Of the 66 success criteria needed to meet WCAG AA, there are none that can be met fully with automated testing alone.

## Usability

It's possible for an app to be WCAG 2.2 Level AA compliant but still have usability issues. Some of these usability issues may also be accessibility issues.

Usability and context matters to everyone, including those with accessibility needs. Usability includes user experience, covering whether the tool is effective and efficient.

Manual accessibility testing, and other functional testing, is needed to make sure your app or services is usable.

## Example: colour contrast

For some success criteria such as those around colour contrast, automated testing tools are vital.

<p>
  <ic-link
    target="_blank"
    href="https://www.w3.org/WAI/WCAG22/quickref/?showtechniques=141#contrast-minimum"
    rel="noreferer noopener nofollow"
  >
    1.4.3 Contrast (Minimum)
  </ic-link>{" "}
  requires the visual presentation of text and images of text to have a contrast
  ratio of at least 4.5:1 (with exceptions).
</p>

An automated testing tool can't test for context so can't tell you if the failing contrast ratio was one of the permitted exceptions, so is actually meeting the success criteria (a false positive).

## Example: alt text

<p>
  <ic-link
    target="_blank"
    href="https://www.w3.org/WAI/WCAG22/quickref/#text-alternatives"
    rel="noreferer noopener nofollow"
  >
    Guideline 1.1 – Text Alternatives
  </ic-link>{" "}
  requires you to provide text alternatives for any non-text content so that it
  can be changed into other forms people need, such as large print, braille,
  speech, symbols or simpler language.
</p>

Most automated accessibility checking tools will warn you if alternative text does not on images. However, automated tools can't tell whether the alternative text provided is useful and providing enough context of the image.

If the alternative text was a file name, such as `alt="picture1234.jpg"`, this would make no sense to the reader.

Likewise, an overly visual description of your app logo does does not provide the reader with the brand recognition, whereas the app name would.

The same image could also be used twice in the same page, but with different purposes, so the alternative text would need to reflect this.
